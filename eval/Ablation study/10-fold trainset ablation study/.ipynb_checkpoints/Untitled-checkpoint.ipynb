{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"../../../utils\")\n",
    "from labels import label2id, id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\r\n",
      "\r\n",
      "           0    0.00000   0.00000   0.00000       144\r\n",
      "           1    0.00000   0.00000   0.00000       294\r\n",
      "           2    0.00000   0.00000   0.00000        72\r\n",
      "           3    0.00000   0.00000   0.00000       107\r\n",
      "           4    0.00000   0.00000   0.00000       209\r\n",
      "           5    0.00000   0.00000   0.00000       493\r\n",
      "           6    0.00000   0.00000   0.00000       466\r\n",
      "           7    0.00000   0.00000   0.00000       229\r\n",
      "           8    0.34639   1.00000   0.51454      2123\r\n",
      "           9    0.00000   0.00000   0.00000      1058\r\n",
      "          10    0.00000   0.00000   0.00000       621\r\n",
      "          11    0.00000   0.00000   0.00000       129\r\n",
      "          12    0.00000   0.00000   0.00000        76\r\n",
      "          13    0.00000   0.00000   0.00000       108\r\n",
      "\r\n",
      "    accuracy                        0.34639      6129\r\n",
      "   macro avg    0.02474   0.07143   0.03675      6129\r\n",
      "weighted avg    0.11998   0.34639   0.17823      6129"
     ]
    }
   ],
   "source": [
    "! cat ../testing\\ other\\ simple\\ models/baseline_trainset.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [file for file in os.listdir() if file.split(\".\")[-1] == \"txt\"] + [\"../testing other simple models/baseline_trainset.txt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['full_model.txt', 'no_bertlogits.txt', 'no_finetuning.txt', 'no_features.txt', 'no_logreg.txt', 'no_features_no_logreg.txt', \"../testing other simple models/baseline_trainset.txt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_f1(file_name):\n",
    "    with open(file_name, \"r\") as fp:\n",
    "        lines = fp.readlines()\n",
    "    #display(lines)\n",
    "    f1 = [row.split()[3] for row in lines[2:16]] + [lines[17].split()[1]]\n",
    "    # f1 += [row.split()[4] for row in lines[18:]]\n",
    "    return f1\n",
    "\n",
    "def create_f1_df(files):\n",
    "    results = pd.DataFrame()\n",
    "    for file_name in files:\n",
    "        model = file_name.split(\".\")[-2].split(\"/\")[-1]\n",
    "        f1s = get_class_f1(file_name)\n",
    "        results[model] = f1s\n",
    "    results.index = [id2label[x] for x in results.index[:14]] + [\"micro f1\"] #, \"macro f1\", \"weigthed f1\"]\n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = create_f1_df(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['word_resemble_factor', 'HCF & LR', 'BERT',\n",
    "       'article_one_word_counter', 'Full model', 'LR',\n",
    "       'article_span_sentence_counter', 'span_word_length',\n",
    "       'word_count_span_sent', 'HCF', 'finetuning',\n",
    "       'baseline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['baseline', 'Full model', 'BERT', 'LR', 'HCF', 'HCF & LR', 'finetuning', 'word_resemble_factor', 'article_one_word_counter','article_span_sentence_counter', 'span_word_length','word_count_span_sent']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrrrrrrrrr}\n",
      "\\toprule\n",
      "{} & baseline & Full model & BERT & LR & HCF & HCF \\& LR & finetuning & word\\_resemble\\_factor & article\\_one\\_word\\_counter & article\\_span\\_sentence\\_counter & span\\_word\\_length & word\\_count\\_span\\_sent \\\\\n",
      "\\midrule\n",
      "Appeal\\_to\\_Authority & 0.000 & 0.341 & 0.014 & 0.316 & 0.308 & 0.304 & 0.049 & 0.342 & 0.357 &  0.323 & 0.293 & 0.351 \\\\\n",
      "Appeal\\_to\\_fear-prejudice & 0.000 & 0.447 & 0.019 & 0.462 & 0.442 & 0.452 & 0.163 & 0.475 & 0.455 &  0.472 & 0.448 & 0.456 \\\\\n",
      "Bandwagon,Reductio\\_ad\\_hitlerum & 0.000 & 0.162 & 0.000 & 0.158 & 0.204 & 0.214 & 0.000 & 0.152 & 0.168 &  0.204 & 0.160 & 0.174 \\\\\n",
      "Black-and-White\\_Fallacy & 0.000 & 0.200 & 0.000 & 0.198 & 0.279 & 0.255 & 0.000 & 0.190 & 0.236 &  0.173 & 0.281 & 0.240 \\\\\n",
      "Causal\\_Oversimplification & 0.000 & 0.433 & 0.000 & 0.405 & 0.424 & 0.441 & 0.056 & 0.425 & 0.456 &  0.434 & 0.451 & 0.437 \\\\\n",
      "Doubt  & 0.000 & 0.640 & 0.384 & 0.642 & 0.639 & 0.633 & 0.426 & 0.643 & 0.650 &  0.638 & 0.636 & 0.636 \\\\\n",
      "Exaggeration,Minimisation & 0.000 & 0.530 & 0.000 & 0.535 & 0.532 & 0.546 & 0.152 & 0.531 & 0.525 &  0.524 & 0.526 & 0.528 \\\\\n",
      "Flag-Waving & 0.000 & 0.622 & 0.000 & 0.612 & 0.607 & 0.606 & 0.373 & 0.617 & 0.633 &  0.620 & 0.604 & 0.622 \\\\\n",
      "Loaded\\_Language & 0.515 & 0.796 & 0.630 & 0.793 & 0.796 & 0.793 & 0.634 & 0.793 & 0.790 &  0.789 & 0.790 & 0.796 \\\\\n",
      "Name\\_Calling,Labeling & 0.000 & 0.792 & 0.338 & 0.790 & 0.787 & 0.789 & 0.403 & 0.787 & 0.786 &  0.785 & 0.785 & 0.788 \\\\\n",
      "Repetition  & 0.000 & 0.646 & 0.541 & 0.638 & 0.619 & 0.615 & 0.543 & 0.638 & 0.621 &  0.634 & 0.638 & 0.638 \\\\\n",
      "Slogans  & 0.000 & 0.518 & 0.000 & 0.514 & 0.520 & 0.530 & 0.172 & 0.516 & 0.530 &  0.510 & 0.526 & 0.529 \\\\\n",
      "Thought-terminating\\_Cliches & 0.000 & 0.343 & 0.000 & 0.318 & 0.341 & 0.331 & 0.000 & 0.284 & 0.321 &  0.293 & 0.321 & 0.299 \\\\\n",
      "Whataboutism,Straw\\_Men,Red\\_Herring & 0.000 & 0.157 & 0.000 & 0.156 & 0.141 & 0.162 & 0.000 & 0.173 & 0.144 &  0.131 & 0.127 & 0.101 \\\\\n",
      "micro f1  & 0.346 & 0.672 & 0.443 & 0.669 & 0.668 & 0.667 & 0.467 & 0.671 & 0.670 &  0.669 & 0.667 & 0.671 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(str(df.astype(float).round(3).to_latex()).replace(\"   \",\" \").replace(\"  \",\" \").replace(\"  \",\" \").replace(\"  \",\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"test\"] = f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'|c|c|c|c|c|c|c|c|c|c|c|c|c'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"lllllllllllll\".replace(\"l\", \"|c\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>baseline</th>\n",
       "      <th>Full model</th>\n",
       "      <th>BERT</th>\n",
       "      <th>LR</th>\n",
       "      <th>HCF</th>\n",
       "      <th>HCF &amp; LR</th>\n",
       "      <th>finetuning</th>\n",
       "      <th>word_resemble_factor</th>\n",
       "      <th>article_one_word_counter</th>\n",
       "      <th>article_span_sentence_counter</th>\n",
       "      <th>span_word_length</th>\n",
       "      <th>word_count_span_sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Appeal_to_Authority</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Appeal_to_fear-prejudice</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bandwagon,Reductio_ad_hitlerum</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Black-and-White_Fallacy</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Causal_Oversimplification</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doubt</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Exaggeration,Minimisation</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Flag-Waving</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Loaded_Language</th>\n",
       "      <td>0.51</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Name_Calling,Labeling</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Repetition</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Slogans</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Thought-terminating_Cliches</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Whataboutism,Straw_Men,Red_Herring</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>micro f1</th>\n",
       "      <td>0.35</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    baseline  Full model  BERT    LR   HCF  \\\n",
       "Appeal_to_Authority                     0.00        0.34  0.01  0.32  0.31   \n",
       "Appeal_to_fear-prejudice                0.00        0.45  0.02  0.46  0.44   \n",
       "Bandwagon,Reductio_ad_hitlerum          0.00        0.16  0.00  0.16  0.20   \n",
       "Black-and-White_Fallacy                 0.00        0.20  0.00  0.20  0.28   \n",
       "Causal_Oversimplification               0.00        0.43  0.00  0.40  0.42   \n",
       "Doubt                                   0.00        0.64  0.38  0.64  0.64   \n",
       "Exaggeration,Minimisation               0.00        0.53  0.00  0.54  0.53   \n",
       "Flag-Waving                             0.00        0.62  0.00  0.61  0.61   \n",
       "Loaded_Language                         0.51        0.80  0.63  0.79  0.80   \n",
       "Name_Calling,Labeling                   0.00        0.79  0.34  0.79  0.79   \n",
       "Repetition                              0.00        0.65  0.54  0.64  0.62   \n",
       "Slogans                                 0.00        0.52  0.00  0.51  0.52   \n",
       "Thought-terminating_Cliches             0.00        0.34  0.00  0.32  0.34   \n",
       "Whataboutism,Straw_Men,Red_Herring      0.00        0.16  0.00  0.16  0.14   \n",
       "micro f1                                0.35        0.67  0.44  0.67  0.67   \n",
       "\n",
       "                                    HCF & LR  finetuning  \\\n",
       "Appeal_to_Authority                     0.30        0.05   \n",
       "Appeal_to_fear-prejudice                0.45        0.16   \n",
       "Bandwagon,Reductio_ad_hitlerum          0.21        0.00   \n",
       "Black-and-White_Fallacy                 0.25        0.00   \n",
       "Causal_Oversimplification               0.44        0.06   \n",
       "Doubt                                   0.63        0.43   \n",
       "Exaggeration,Minimisation               0.55        0.15   \n",
       "Flag-Waving                             0.61        0.37   \n",
       "Loaded_Language                         0.79        0.63   \n",
       "Name_Calling,Labeling                   0.79        0.40   \n",
       "Repetition                              0.61        0.54   \n",
       "Slogans                                 0.53        0.17   \n",
       "Thought-terminating_Cliches             0.33        0.00   \n",
       "Whataboutism,Straw_Men,Red_Herring      0.16        0.00   \n",
       "micro f1                                0.67        0.47   \n",
       "\n",
       "                                    word_resemble_factor  \\\n",
       "Appeal_to_Authority                                 0.34   \n",
       "Appeal_to_fear-prejudice                            0.47   \n",
       "Bandwagon,Reductio_ad_hitlerum                      0.15   \n",
       "Black-and-White_Fallacy                             0.19   \n",
       "Causal_Oversimplification                           0.42   \n",
       "Doubt                                               0.64   \n",
       "Exaggeration,Minimisation                           0.53   \n",
       "Flag-Waving                                         0.62   \n",
       "Loaded_Language                                     0.79   \n",
       "Name_Calling,Labeling                               0.79   \n",
       "Repetition                                          0.64   \n",
       "Slogans                                             0.52   \n",
       "Thought-terminating_Cliches                         0.28   \n",
       "Whataboutism,Straw_Men,Red_Herring                  0.17   \n",
       "micro f1                                            0.67   \n",
       "\n",
       "                                    article_one_word_counter  \\\n",
       "Appeal_to_Authority                                     0.36   \n",
       "Appeal_to_fear-prejudice                                0.46   \n",
       "Bandwagon,Reductio_ad_hitlerum                          0.17   \n",
       "Black-and-White_Fallacy                                 0.24   \n",
       "Causal_Oversimplification                               0.46   \n",
       "Doubt                                                   0.65   \n",
       "Exaggeration,Minimisation                               0.53   \n",
       "Flag-Waving                                             0.63   \n",
       "Loaded_Language                                         0.79   \n",
       "Name_Calling,Labeling                                   0.79   \n",
       "Repetition                                              0.62   \n",
       "Slogans                                                 0.53   \n",
       "Thought-terminating_Cliches                             0.32   \n",
       "Whataboutism,Straw_Men,Red_Herring                      0.14   \n",
       "micro f1                                                0.67   \n",
       "\n",
       "                                    article_span_sentence_counter  \\\n",
       "Appeal_to_Authority                                          0.32   \n",
       "Appeal_to_fear-prejudice                                     0.47   \n",
       "Bandwagon,Reductio_ad_hitlerum                               0.20   \n",
       "Black-and-White_Fallacy                                      0.17   \n",
       "Causal_Oversimplification                                    0.43   \n",
       "Doubt                                                        0.64   \n",
       "Exaggeration,Minimisation                                    0.52   \n",
       "Flag-Waving                                                  0.62   \n",
       "Loaded_Language                                              0.79   \n",
       "Name_Calling,Labeling                                        0.79   \n",
       "Repetition                                                   0.63   \n",
       "Slogans                                                      0.51   \n",
       "Thought-terminating_Cliches                                  0.29   \n",
       "Whataboutism,Straw_Men,Red_Herring                           0.13   \n",
       "micro f1                                                     0.67   \n",
       "\n",
       "                                    span_word_length  word_count_span_sent  \n",
       "Appeal_to_Authority                             0.29                  0.35  \n",
       "Appeal_to_fear-prejudice                        0.45                  0.46  \n",
       "Bandwagon,Reductio_ad_hitlerum                  0.16                  0.17  \n",
       "Black-and-White_Fallacy                         0.28                  0.24  \n",
       "Causal_Oversimplification                       0.45                  0.44  \n",
       "Doubt                                           0.64                  0.64  \n",
       "Exaggeration,Minimisation                       0.53                  0.53  \n",
       "Flag-Waving                                     0.60                  0.62  \n",
       "Loaded_Language                                 0.79                  0.80  \n",
       "Name_Calling,Labeling                           0.78                  0.79  \n",
       "Repetition                                      0.64                  0.64  \n",
       "Slogans                                         0.53                  0.53  \n",
       "Thought-terminating_Cliches                     0.32                  0.30  \n",
       "Whataboutism,Straw_Men,Red_Herring              0.13                  0.10  \n",
       "micro f1                                        0.67                  0.67  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.astype(float).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
